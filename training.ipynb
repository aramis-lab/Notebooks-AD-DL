{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains how to train a CNN on OASIS data that was processed in the previous sections. However if you do not have an access to a GPU, training the CNN will require too much time, hence pretrained models will be given to execute the last part of the course (inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available', torch.cuda.is_available())\n",
    "\n",
    "# You can execute this notebook on Colab to run it on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the goal is to train a classifier to differentiate the **AD label from the CN label** based on the associated **T1-MR images**. These images were preprocessed according to the <font color='green'>appropriate section</font>.\n",
    "\n",
    "According to the literature review done in <font color='green'>give here the reference of AD-DL</font>, 4 main paradigms were implemented to train a CNN to perform a classification task between AD and CN labels based on T1-MRI. These paradigms depend of the input given to the network:\n",
    "* 2D slices,\n",
    "* 3D patches,\n",
    "* ROIs (Regions of Interest).\n",
    "* 3D images.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Computational issues:</b><p>\n",
    "    Depending on your computational ressources, you should adapt the parameters in <b>COMPUTATIONAL ISSUES</b> group:\n",
    "    <ul>\n",
    "        <li> -gpu flag allows the system to use a GPU if available. </li>\n",
    "        <li> --nproc N sets the number of workers (parallel processes) in the Dataloader to N. </li>\n",
    "        <li> --batch_size B will set the batch size to B. If the batch size is too high, it can raise memory errors. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D slice-level CNN\n",
    "\n",
    "The 2D slice-level CNN in `clinicadl` is a slight modification of the ResNet-18 network used to train natural images on the ImageNet dataset:\n",
    "* One fully-connected layer was added at the end of the network to reduce the dimension of the output from 1000 to 2 classes <font color='purple'>(purple dotted box)</font>.\n",
    "* The last five convolutional layers and the last FC of ResNet are fine-tuned <font color='green'>(green dotted box)</font>.\n",
    "* All other layers have their weights and biases fixed.\n",
    "\n",
    "<img src=\"./images/2DCNN.png\">\n",
    "\n",
    "During training, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the slices of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>The layers of this network (except the last one) are initialized with the weights of the ResNet-18 trained on ImageNet.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl train slice -h\n",
    "#!clinicadl train slice <caps_dir> data/labels_list results/slice_cnn resnet18 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-patch level & ROI-based models\n",
    "\n",
    "The 3D patch-level and ROI-based CNNs in `clinicadl` have the same architecture, including:\n",
    "* 4 convolutional layers with kernel 3x3x3,\n",
    "* 4 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "For 3D-patch level networks, it is possible to train one CNN per patch location. In this case use the flag `--network_type \"multi\"` and specify the number of patches in `--num_cnn`.\n",
    "\n",
    "<img src=\"./images/ROICNN.png\">\n",
    "\n",
    "As for the 2D slice-level model, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the patches or the two hippocampi of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture. The encoder will share the same architecture than the CNN until the fully-connected layers.</p>\n",
    "    <img src=\"./images/autoencoder.png\">\n",
    "    <p>Then the weights of the encoder will be transferred in the convolutions of the CNN to initialize it before its training. This procedure is called <i>autoencoder pretraining</i>.</p>\n",
    "    <p>It is also possible to transfer weights between two CNNs with the same architecture.</p>\n",
    "    <p>For 3D-patch multi-CNNs specifically, it is possible to initialize each CNN of a multi-CNN:\n",
    "        <ul>\n",
    "        <li> with the weights of a single-CNN,</li>\n",
    "        <li> with the weights of the corresponding CNN in a multi-CNN. </li>\n",
    "    </ul>\n",
    "    <p>Transferring weights between CNNs can be useful when performing two classification tasks that are similar. This is what has been done in <font color='green'>give here the reference of AD-DL</font>: the sMCI vs pMCI classification network was initialized with the weights of the AD vs CN classification network.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>Transferring weights between tasks that are not similar enough can hurt the performance !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D-patch level models\n",
    "\n",
    "See definition of patches in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch autoencoder pretraining\n",
    "!clinicadl train patch autoencoder -h\n",
    "#!clinicadl train patch autoencoder <caps_dir> data/labels_list results/patch_autoencoder Conv4_FC3 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch single-CNN training\n",
    "!clinicadl train patch cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_single-cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_single-cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch multi-CNN training\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# With single-CNN pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl --transfer_learning_path results/patch_single-cnn\n",
    "# Without pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI-based models\n",
    "\n",
    "See definition of ROI in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based autoencoder pretraining\n",
    "!clinicadl train roi autoencoder -h\n",
    "#!clinicadl train roi autoencoder <caps_dir> data/labels_list results/roi_autoencoder Conv4_FC3 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based CNN training\n",
    "!clinicadl train roi cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> data/labels_list results/roi_cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl --transfer_learning_autoencoder --transfer_learning_path results/roi_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> data/labels_list results/roi_cnn Conv4_FC3 -gpu --batch_size 16 --prepare_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-image level model\n",
    "\n",
    "The 3D image-level CNN in `clinicadl` is designed as follows:\n",
    "* 5 convolutional layers with kernel 3x3x3,\n",
    "* 5 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "<img src=\"./images/imageCNN.png\">\n",
    "\n",
    "Depending on the preprocessing, the size of the fully connected layers must be adapted. This is why two models exist in `clinicadl`:\n",
    "* `Conv5_FC3` adapted to `linear` preprocessing, <font color=\"green\">names might change</font>\n",
    "* `Conv5_FC3_mni` adapted to `mni` preprocessing.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture, or to transfer weights between CNNs. See the section on patches for more details on this topic !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image autoencoder -h\n",
    "#!clinicadl train image autoencoder <caps_dir> data/labels_list results/image_autoencoder Conv5_FC3 -gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train image cnn <caps_dir> data/labels_list results/image_cnn Conv5_FC3 -n -gpu --transfer_learning_autoencoder --transfer_learning_path results/image_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train image cnn <caps_dir> data/labels_list results/image_cnn Conv5_FC3 -n -gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models\n",
    "\n",
    "You want to add your custom architecture and train it to clinicadl ? Please fork and clone the [github repo](https://github.com/aramis-lab/AD-DL]) and code your model in pytorch in `clinicadl/tools/deep_learning/models` and import it in `clinicadl/tools/deep_learning/models/__init__.py` !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
