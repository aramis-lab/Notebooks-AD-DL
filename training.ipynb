{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains how to train a CNN on OASIS data that was processed in the previous sections. However if you do not have an access to a GPU, training the CNN will require too much time, this is why pretrained models has be given to execute the previous part of the course (inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available', torch.cuda.is_available())\n",
    "\n",
    "# You can execute this notebook on Colab to run it on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the goal is to train a classifier to differentiate the **AD label from the CN label** based on the associated **T1-MR images**. These images were preprocessed according to the <font color='green'>appropriate section</font>.\n",
    "\n",
    "According to the literature review done in <font color='green'>give here the reference of AD-DL</font>, 4 main paradigms were implemented to train a CNN to perform a classification task between AD and CN labels based on T1-MRI. These paradigms depend of the input given to the network:\n",
    "* 2D slices,\n",
    "* 3D patches,\n",
    "* ROIs (Regions of Interest).\n",
    "* 3D images.\n",
    "\n",
    "There is no consensus in the literature on the architecture of the models that should be used for each input category. Part of the studies of the bibliography used custom models, and the other reused architectures that led to good results on natural images (VGGNet, ResNet, GoogleNet, DenseNet...). We chose to find custom architectures by running a search on the main architecture components (more details in <font color='green'>give here the reference of relevant section of AD-DL</font>). This architecture search was run only on the train + validation sets, and **no test sets were used to choose the best architecture**. This is crucial, as the use of a test set during architecture search can lead to over-optimistic results.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Computational issues:</b><p>\n",
    "    Depending on your computational ressources, you should adapt the parameters in <b>COMPUTATIONAL ISSUES</b> group:\n",
    "    <ul>\n",
    "        <li> -cpu flag forces the system to use CPU instead of looking for a GPU. </li>\n",
    "        <li> --nproc N sets the number of workers (parallel processes) in the Dataloader to N. </li>\n",
    "        <li> --batch_size B will set the batch size to B. If the batch size is too high, it can raise memory errors. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D slice-level CNN\n",
    "\n",
    "An advantage of this approach is that existing CNNs which had huge success for natural image classification, e.g. ResNet [(He et al., 2016)](https://doi.org/10.1109/CVPR.2016.90) and VGGNet [(Simonyan and Zisserman, 2014)](https://arxiv.org/abs/1409.1556) , can be easily borrowed and used in a transfer learning fashion. Other advantages are the increased number of training samples as many slices can be extracted from a single 3D image, and a lower memory usage compared to using the full MR image as input.\n",
    "\n",
    "The 2D slice-level CNN in `clinicadl` is a slight modification of the ResNet-18 network used to train natural images on the ImageNet dataset:\n",
    "* One fully-connected layer was added at the end of the network to reduce the dimension of the output from 1000 to 2 classes <font color='purple'>(purple dotted box)</font>.\n",
    "* The last five convolutional layers and the last FC of ResNet are fine-tuned <font color='green'>(green dotted box)</font>.\n",
    "* All other layers have their weights and biases fixed.\n",
    "\n",
    "<img src=\"./images/2DCNN.png\">\n",
    "\n",
    "During training, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the slices of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>The layers of this network (except the last one) are initialized with the weights of the ResNet-18 trained on ImageNet.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: clinicadl train slice [-h] [-cpu] [-np NPROC] [--batch_size BATCH_SIZE]\r\n",
      "                             [--diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]]\r\n",
      "                             [--baseline] [--unnormalize]\r\n",
      "                             [--n_splits N_SPLITS] [--split SPLIT [SPLIT ...]]\r\n",
      "                             [--epochs EPOCHS] [--learning_rate LEARNING_RATE]\r\n",
      "                             [--weight_decay WEIGHT_DECAY] [--dropout DROPOUT]\r\n",
      "                             [--patience PATIENCE] [--tolerance TOLERANCE]\r\n",
      "                             [--evaluation_steps EVALUATION_STEPS]\r\n",
      "                             [--accumulation_steps ACCUMULATION_STEPS]\r\n",
      "                             [--slice_direction SLICE_DIRECTION]\r\n",
      "                             [--discarded_slices DISCARDED_SLICES [DISCARDED_SLICES ...]]\r\n",
      "                             [--use_extracted_slices]\r\n",
      "                             [--selection_threshold SELECTION_THRESHOLD]\r\n",
      "                             caps_dir {t1-linear,t1-extensive} tsv_path\r\n",
      "                             output_dir model\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "\u001b[34mPositional arguments\u001b[39m:\r\n",
      "  caps_dir              Data using CAPS structure.\r\n",
      "  {t1-linear,t1-extensive}\r\n",
      "                        Defines the type of preprocessing of CAPS data.\r\n",
      "  tsv_path              TSV path with subjects/sessions to process.\r\n",
      "  output_dir            Folder containing results of the training.\r\n",
      "  model                 CNN Model to be used during the training.\r\n",
      "\r\n",
      "\u001b[34mComputational resources\u001b[39m:\r\n",
      "  -cpu, --use_cpu       Uses CPU instead of GPU.\r\n",
      "  -np NPROC, --nproc NPROC\r\n",
      "                        Number of cores used during the training.\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "                        Batch size for training. (default=2)\r\n",
      "\r\n",
      "\u001b[34mData management\u001b[39m:\r\n",
      "  --diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...], -d {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]\r\n",
      "                        Diagnoses that will be selected for training.\r\n",
      "  --baseline            if True only the baseline is used.\r\n",
      "  --unnormalize, -un    Disable default MinMaxNormalization.\r\n",
      "\r\n",
      "\u001b[34mCross-validation arguments\u001b[39m:\r\n",
      "  --n_splits N_SPLITS   If a value is given will load data of a k-fold CV.\r\n",
      "  --split SPLIT [SPLIT ...]\r\n",
      "                        Train the list of given folds. By default train all\r\n",
      "                        folds.\r\n",
      "\r\n",
      "\u001b[34mOptimization parameters\u001b[39m:\r\n",
      "  --epochs EPOCHS       Epochs through the data. (default=20)\r\n",
      "  --learning_rate LEARNING_RATE, -lr LEARNING_RATE\r\n",
      "                        Learning rate of the optimization. (default=0.01)\r\n",
      "  --weight_decay WEIGHT_DECAY, -wd WEIGHT_DECAY\r\n",
      "                        Weight decay value used in optimization.\r\n",
      "                        (default=1e-4)\r\n",
      "  --dropout DROPOUT     rate of dropout that will be applied to dropout\r\n",
      "                        layers.\r\n",
      "  --patience PATIENCE   Waiting time for early stopping.\r\n",
      "  --tolerance TOLERANCE\r\n",
      "                        Tolerance value for the early stopping.\r",
      "\r\n",
      "  --evaluation_steps EVALUATION_STEPS, -esteps EVALUATION_STEPS\r\n",
      "                        Fix the number of batches to use before validation.\r\n",
      "  --accumulation_steps ACCUMULATION_STEPS, -asteps ACCUMULATION_STEPS\r\n",
      "                        Accumulates gradients in order to increase the size of\r\n",
      "                        the batch.\r\n",
      "\r\n",
      "\u001b[34mSlice-level parameters\u001b[39m:\r\n",
      "  --slice_direction SLICE_DIRECTION, -sd SLICE_DIRECTION\r\n",
      "                        Which coordinate axis to take for slicing the MRI. 0\r\n",
      "                        for sagittal 1 for coronal 2 for axial direction.\r\n",
      "  --discarded_slices DISCARDED_SLICES [DISCARDED_SLICES ...]\r\n",
      "                        Number of slices discarded from respectively the\r\n",
      "                        beginning and the end of the MRI volume. If only one\r\n",
      "                        argument is given, it will be used for both sides.\r\n",
      "  --use_extracted_slices\r\n",
      "                        If True the outputs of extract preprocessing are used,\r\n",
      "                        else the whole MRI is loaded.\r\n",
      "  --selection_threshold SELECTION_THRESHOLD\r\n",
      "                        Threshold on the balanced accuracies to compute the\r\n",
      "                        subject-level performance. Slices are selected if\r\n",
      "                        their balanced accuracy > threshold. Default\r\n",
      "                        corresponds to no selection.\r\n"
     ]
    }
   ],
   "source": [
    "!clinicadl train slice -h\n",
    "#!clinicadl train slice <caps_dir> \"t1-linear\" data/labels_list results/slice_cnn resnet18 --batch_size 16 --use_extracted_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-patch level & ROI-based models\n",
    "\n",
    "The 3D patch-level models compensate the absence of 3D information in the 2D slice-level approach and keep some of its advantages (low memory usage and larger sample size). This paradigm can be divided in two different frameworks:\n",
    "- `single-CNN`: one CNN is trained on all patch locations.\n",
    "- `multi-CNN`: one CNN is trained per patch location.\n",
    "For `multi-CNN` the sample size is smaller (equivalent to image level framework), however the CNNs may be more accurate as they are specialized in one patch location.\n",
    "\n",
    "ROI-based models are similar to 3D-patch but take advantage of prior knowledge on Alzheimer's disease. Indeed most of the patches are not informative as they contain parts of the brain that are not affected by the disease. Methods based on regions of interest (ROI) overcome this issue by focusing on regions which are known to be informative: the hippocampi. In this way, the complexity of the framework can be decreased as fewer inputs are used to train the networks.\n",
    "\n",
    "The 3D patch-level and ROI-based CNNs in `clinicadl` have the same architecture, including:\n",
    "* 4 convolutional layers with kernel 3x3x3,\n",
    "* 4 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "For 3D-patch level networks, it is possible to train one CNN per patch location. In this case use the subparser `multicnn` and specify the number of patches in `--num_cnn`.\n",
    "\n",
    "<img src=\"./images/ROICNN.png\">\n",
    "\n",
    "As for the 2D slice-level model, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the patches or the two hippocampi of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture. The encoder will share the same architecture than the CNN until the fully-connected layers.</p>\n",
    "    <img src=\"./images/autoencoder.png\">\n",
    "    <p>Then the weights of the encoder will be transferred in the convolutions of the CNN to initialize it before its training. This procedure is called <i>autoencoder pretraining</i>.</p>\n",
    "    <p>It is also possible to transfer weights between two CNNs with the same architecture.</p>\n",
    "    <p>For 3D-patch multi-CNNs specifically, it is possible to initialize each CNN of a multi-CNN:\n",
    "        <ul>\n",
    "        <li> with the weights of a single-CNN,</li>\n",
    "        <li> with the weights of the corresponding CNN in a multi-CNN. </li>\n",
    "    </ul>\n",
    "    <p>Transferring weights between CNNs can be useful when performing two classification tasks that are similar. This is what has been done in <font color='green'>give here the reference of AD-DL</font>: the sMCI vs pMCI classification network was initialized with the weights of the AD vs CN classification network.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>Transferring weights between tasks that are not similar enough can hurt the performance !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D-patch level models\n",
    "\n",
    "See definition of patches in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: clinicadl train patch autoencoder [-h] [-cpu] [-np NPROC]\r\n",
      "                                         [--batch_size BATCH_SIZE]\r\n",
      "                                         [--diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]]\r\n",
      "                                         [--baseline] [--unnormalize]\r\n",
      "                                         [--n_splits N_SPLITS]\r\n",
      "                                         [--split SPLIT [SPLIT ...]]\r\n",
      "                                         [--epochs EPOCHS]\r\n",
      "                                         [--learning_rate LEARNING_RATE]\r\n",
      "                                         [--weight_decay WEIGHT_DECAY]\r\n",
      "                                         [--dropout DROPOUT]\r\n",
      "                                         [--patience PATIENCE]\r\n",
      "                                         [--tolerance TOLERANCE]\r\n",
      "                                         [--evaluation_steps EVALUATION_STEPS]\r\n",
      "                                         [--accumulation_steps ACCUMULATION_STEPS]\r\n",
      "                                         [-ps PATCH_SIZE] [-ss STRIDE_SIZE]\r\n",
      "                                         [--use_extracted_patches]\r\n",
      "                                         [--visualization]\r\n",
      "                                         caps_dir {t1-linear,t1-extensive}\r\n",
      "                                         tsv_path output_dir model\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "\u001b[34mPositional arguments\u001b[39m:\r\n",
      "  caps_dir              Data using CAPS structure.\r\n",
      "  {t1-linear,t1-extensive}\r\n",
      "                        Defines the type of preprocessing of CAPS data.\r\n",
      "  tsv_path              TSV path with subjects/sessions to process.\r\n",
      "  output_dir            Folder containing results of the training.\r\n",
      "  model                 CNN Model to be used during the training.\r\n",
      "\r\n",
      "\u001b[34mComputational resources\u001b[39m:\r\n",
      "  -cpu, --use_cpu       Uses CPU instead of GPU.\r\n",
      "  -np NPROC, --nproc NPROC\r\n",
      "                        Number of cores used during the training.\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "                        Batch size for training. (default=2)\r\n",
      "\r\n",
      "\u001b[34mData management\u001b[39m:\r\n",
      "  --diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...], -d {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]\r\n",
      "                        Diagnoses that will be selected for training.\r\n",
      "  --baseline            if True only the baseline is used.\r\n",
      "  --unnormalize, -un    Disable default MinMaxNormalization.\r\n",
      "\r\n",
      "\u001b[34mCross-validation arguments\u001b[39m:\r\n",
      "  --n_splits N_SPLITS   If a value is given will load data of a k-fold CV.\r\n",
      "  --split SPLIT [SPLIT ...]\r\n",
      "                        Train the list of given folds. By default train all\r\n",
      "                        folds.\r\n",
      "\r\n",
      "\u001b[34mOptimization parameters\u001b[39m:\r\n",
      "  --epochs EPOCHS       Epochs through the data. (default=20)\r\n",
      "  --learning_rate LEARNING_RATE, -lr LEARNING_RATE\r\n",
      "                        Learning rate of the optimization. (default=0.01)\r\n",
      "  --weight_decay WEIGHT_DECAY, -wd WEIGHT_DECAY\r\n",
      "                        Weight decay value used in optimization.\r\n",
      "                        (default=1e-4)\r\n",
      "  --dropout DROPOUT     rate of dropout that will be applied to dropout\r\n",
      "                        layers.\r\n",
      "  --patience PATIENCE   Waiting time for early stopping.\r\n",
      "  --tolerance TOLERANCE\r\n",
      "                        Tolerance value for the early stopping.\r\n",
      "  --evaluation_steps EVALUATION_STEPS, -esteps EVALUATION_STEPS\r\n",
      "                        Fix the number of batches to use before validation.\r\n",
      "  --accumulation_steps ACCUMULATION_STEPS, -asteps ACCUMULATION_STEPS\r\n",
      "                        Accumulates gradients in order to increase the size of\r\n",
      "                        the batch.\r\n",
      "\r\n",
      "\u001b[34mPatch-level parameters\u001b[39m:\r\n",
      "  -ps PATCH_SIZE, --patch_size PATCH_SIZE\r\n",
      "                        Patch size\r\n",
      "  -ss STRIDE_SIZE, --stride_size STRIDE_SIZE\r\n",
      "                        Stride size\r\n",
      "  --use_extracted_patches\r\n",
      "                        If True the outputs of extract preprocessing are used,\r\n",
      "                        else the whole MRI is loaded.\r\n",
      "\r\n",
      "\u001b[34mAutoencoder specific\u001b[39m:\r\n",
      "  --visualization       Save results in visualization folder.\r\n"
     ]
    }
   ],
   "source": [
    "# 3D-patch autoencoder pretraining\n",
    "!clinicadl train patch autoencoder -h\n",
    "#!clinicadl train patch autoencoder <caps_dir> \"t1-linear\" data/labels_list results/patch_autoencoder Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: clinicadl train patch cnn [-h] [-cpu] [-np NPROC]\r\n",
      "                                 [--batch_size BATCH_SIZE]\r\n",
      "                                 [--diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]]\r\n",
      "                                 [--baseline] [--unnormalize]\r\n",
      "                                 [--n_splits N_SPLITS]\r\n",
      "                                 [--split SPLIT [SPLIT ...]] [--epochs EPOCHS]\r\n",
      "                                 [--learning_rate LEARNING_RATE]\r\n",
      "                                 [--weight_decay WEIGHT_DECAY]\r\n",
      "                                 [--dropout DROPOUT] [--patience PATIENCE]\r\n",
      "                                 [--tolerance TOLERANCE]\r\n",
      "                                 [--evaluation_steps EVALUATION_STEPS]\r\n",
      "                                 [--accumulation_steps ACCUMULATION_STEPS]\r\n",
      "                                 [-ps PATCH_SIZE] [-ss STRIDE_SIZE]\r\n",
      "                                 [--use_extracted_patches]\r\n",
      "                                 [--transfer_learning_path TRANSFER_LEARNING_PATH]\r\n",
      "                                 [--transfer_learning_autoencoder]\r\n",
      "                                 [--transfer_learning_selection {best_loss,best_acc}]\r\n",
      "                                 [--selection_threshold SELECTION_THRESHOLD]\r\n",
      "                                 caps_dir {t1-linear,t1-extensive} tsv_path\r\n",
      "                                 output_dir model\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "\r\n",
      "\u001b[34mPositional arguments\u001b[39m:\r\n",
      "  caps_dir              Data using CAPS structure.\r\n",
      "  {t1-linear,t1-extensive}\r\n",
      "                        Defines the type of preprocessing of CAPS data.\r\n",
      "  tsv_path              TSV path with subjects/sessions to process.\r\n",
      "  output_dir            Folder containing results of the training.\r\n",
      "  model                 CNN Model to be used during the training.\r\n",
      "\r\n",
      "\u001b[34mComputational resources\u001b[39m:\r\n",
      "  -cpu, --use_cpu       Uses CPU instead of GPU.\r\n",
      "  -np NPROC, --nproc NPROC\r\n",
      "                        Number of cores used during the training.\r\n",
      "  --batch_size BATCH_SIZE\r\n",
      "                        Batch size for training. (default=2)\r\n",
      "\r\n",
      "\u001b[34mData management\u001b[39m:\r\n",
      "  --diagnoses {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...], -d {AD,CN,MCI,sMCI,pMCI} [{AD,CN,MCI,sMCI,pMCI} ...]\r\n",
      "                        Diagnoses that will be selected for training.\r\n",
      "  --baseline            if True only the baseline is used.\r\n",
      "  --unnormalize, -un    Disable default MinMaxNormalization.\r\n",
      "\r\n",
      "\u001b[34mCross-validation arguments\u001b[39m:\r\n",
      "  --n_splits N_SPLITS   If a value is given will load data of a k-fold CV.\r\n",
      "  --split SPLIT [SPLIT ...]\r\n",
      "                        Train the list of given folds. By default train all\r\n",
      "                        folds.\r\n",
      "\r\n",
      "\u001b[34mOptimization parameters\u001b[39m:\r\n",
      "  --epochs EPOCHS       Epochs through the data. (default=20)\r\n",
      "  --learning_rate LEARNING_RATE, -lr LEARNING_RATE\r\n",
      "                        Learning rate of the optimization. (default=0.01)\r\n",
      "  --weight_decay WEIGHT_DECAY, -wd WEIGHT_DECAY\r\n",
      "                        Weight decay value used in optimization.\r\n",
      "                        (default=1e-4)\r\n",
      "  --dropout DROPOUT     rate of dropout that will be applied to dropout\r\n",
      "                        layers.\r\n",
      "  --patience PATIENCE   Waiting time for early stopping.\r\n",
      "  --tolerance TOLERANCE\r\n",
      "                        Tolerance value for the early stopping.\r\n",
      "  --evaluation_steps EVALUATION_STEPS, -esteps EVALUATION_STEPS\r\n",
      "                        Fix the number of batches to use before validation.\r\n",
      "  --accumulation_steps ACCUMULATION_STEPS, -asteps ACCUMULATION_STEPS\r\n",
      "                        Accumulates gradients in order to increase the size of\r\n",
      "                        the batch.\r\n",
      "\r\n",
      "\u001b[34mPatch-level parameters\u001b[39m:\r\n",
      "  -ps PATCH_SIZE, --patch_size PATCH_SIZE\r\n",
      "                        Patch size\r\n",
      "  -ss STRIDE_SIZE, --stride_size STRIDE_SIZE\r\n",
      "                        Stride size\r\n",
      "  --use_extracted_patches\r\n",
      "                        If True the outputs of extract preprocessing are used,\r\n",
      "                        else the whole MRI is loaded.\r\n",
      "\r\n",
      "\u001b[34mTransfer learning\u001b[39m:\r\n",
      "  --transfer_learning_path TRANSFER_LEARNING_PATH\r\n",
      "                        If an existing path is given, a pretrained model is\r\n",
      "                        used.\r\n",
      "  --transfer_learning_autoencoder\r\n",
      "                        If specified, do transfer learning using an\r\n",
      "                        autoencoder else will look for a CNN model.\r\n",
      "  --transfer_learning_selection {best_loss,best_acc}\r\n",
      "                        If transfer_learning from CNN, chooses which best\r\n",
      "                        transfer model is selected.\r\n",
      "\r\n",
      "\u001b[34mPatch-level CNN parameters\u001b[39m:\r\n",
      "  --selection_threshold SELECTION_THRESHOLD\r\n",
      "                        Threshold on the balanced accuracies to compute the\r\n",
      "                        subject-level performance. Patches are selected if\r\n",
      "                        their balanced accuracy > threshold. Default\r\n",
      "                        corresponds to no selection.\r\n"
     ]
    }
   ],
   "source": [
    "# 3D-patch single-CNN training\n",
    "!clinicadl train patch cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> \"t1-linear\" data/labels_list results/patch_single-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> \"t1-linear\" data/labels_list results/patch_single-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch multi-CNN training\n",
    "!clinicadl train patch multicnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> \"t1-linear\" data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# With single-CNN pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> \"t1-linear\" data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_path results/patch_single-cnn\n",
    "# Without pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> \"t1-linear\" data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI-based models\n",
    "\n",
    "See definition of ROI in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based autoencoder pretraining\n",
    "!clinicadl train roi autoencoder -h\n",
    "#!clinicadl train roi autoencoder <caps_dir> \"t1-linear\" data/labels_list results/roi_autoencoder Conv4_FC3 --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based CNN training\n",
    "!clinicadl train roi cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> \"t1-linear\" data/labels_list results/roi_cnn Conv4_FC3 --batch_size 16 --transfer_learning_autoencoder --transfer_learning_path results/roi_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> \"t1-linear\" data/labels_list results/roi_cnn Conv4_FC3 --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-image level model\n",
    "\n",
    "In this approach, the whole MRI is used at once and the classification is performed at the imageg level. The advantage is that the spatial information is fully integrated, and it may allow to discover new knowledge on the disease. However, it requires more computational resources (especially GPU with higher memory capacity).\n",
    "\n",
    "The 3D image-level CNN in `clinicadl` is designed as follows:\n",
    "* 5 convolutional layers with kernel 3x3x3,\n",
    "* 5 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "<img src=\"./images/imageCNN.png\">\n",
    "\n",
    "Depending on the preprocessing, the size of the fully connected layers must be adapted. This is why two models exist in `clinicadl`:\n",
    "* `Conv5_FC3` adapted to `linear` preprocessing, <font color=\"green\">names might change</font>\n",
    "* `Conv5_FC3_mni` adapted to `mni` preprocessing.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture, or to transfer weights between CNNs. See the section on patches for more details on this topic !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image autoencoder -h\n",
    "#!clinicadl train image autoencoder <caps_dir> \"t1-linear\" data/labels_list results/image_autoencoder Conv5_FC3 -gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train image cnn <caps_dir> \"t1-linear\" data/labels_list results/image_cnn Conv5_FC3 --transfer_learning_autoencoder --transfer_learning_path results/image_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train image cnn <caps_dir> \"t1-linear\" data/labels_list results/image_cnn Conv5_FC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models\n",
    "\n",
    "You want to add your custom architecture and train it with clinicadl ? Please fork and clone the [github repo](https://github.com/aramis-lab/AD-DL]) and code your model in pytorch in `clinicadl/tools/deep_learning/models` and import it in `clinicadl/tools/deep_learning/models/__init__.py` !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
