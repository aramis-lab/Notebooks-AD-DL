{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explains how to train a CNN on OASIS data that was processed in the previous sections. However if you do not have an access to a GPU, training the CNN will require too much time, this is why pretrained models has be given to execute the previous part of the course (inference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "print('GPU is available', torch.cuda.is_available())\n",
    "\n",
    "# You can execute this notebook on Colab to run it on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the goal is to train a classifier to differentiate the **AD label from the CN label** based on the associated **T1-MR images**. These images were preprocessed according to the <font color='green'>appropriate section</font>.\n",
    "\n",
    "According to the literature review done in <font color='green'>give here the reference of AD-DL</font>, 4 main paradigms were implemented to train a CNN to perform a classification task between AD and CN labels based on T1-MRI. These paradigms depend of the input given to the network:\n",
    "* 2D slices,\n",
    "* 3D patches,\n",
    "* ROIs (Regions of Interest).\n",
    "* 3D images.\n",
    "\n",
    "There is no consensus in the literature on the architecture of the models that should be used for each input category. Part of the studies of the bibliography used custom models, and the other reused architectures that led to good results on natural images (VGGNet, ResNet, GoogleNet, DenseNet...). We chose to find custom architectures by running a search on the main architecture components (more details in <font color='green'>give here the reference of relevant section of AD-DL</font>). This architecture search was run only on the train + validation sets, and **no test sets were used to choose the best architecture**. This is crucial, as the use of a test set during architecture search can lead to over-optimistic results.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Computational issues:</b><p>\n",
    "    Depending on your computational ressources, you should adapt the parameters in <b>COMPUTATIONAL ISSUES</b> group:\n",
    "    <ul>\n",
    "        <li> -cpu flag forces the system to use CPU instead of looking for a GPU. </li>\n",
    "        <li> --nproc N sets the number of workers (parallel processes) in the Dataloader to N. </li>\n",
    "        <li> --batch_size B will set the batch size to B. If the batch size is too high, it can raise memory errors. </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D slice-level CNN\n",
    "\n",
    "An advantage of this approach is that existing CNNs which had huge success for natural image classification, e.g. ResNet [(He et al., 2016)](https://doi.org/10.1109/CVPR.2016.90) and VGGNet [(Simonyan and Zisserman, 2014)](https://arxiv.org/abs/1409.1556) , can be easily borrowed and used in a transfer learning fashion. Other advantages are the increased number of training samples as many slices can be extracted from a single 3D image, and a lower memory usage compared to using the full MR image as input.\n",
    "\n",
    "The 2D slice-level CNN in `clinicadl` is a slight modification of the ResNet-18 network used to train natural images on the ImageNet dataset:\n",
    "* One fully-connected layer was added at the end of the network to reduce the dimension of the output from 1000 to 2 classes <font color='purple'>(purple dotted box)</font>.\n",
    "* The last five convolutional layers and the last FC of ResNet are fine-tuned <font color='green'>(green dotted box)</font>.\n",
    "* All other layers have their weights and biases fixed.\n",
    "\n",
    "<img src=\"./images/2DCNN.png\">\n",
    "\n",
    "During training, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the slices of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>The layers of this network (except the last one) are initialized with the weights of the ResNet-18 trained on ImageNet.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinicadl train slice -h\n",
    "#!clinicadl train slice <caps_dir> data/labels_list results/slice_cnn resnet18 --batch_size 16 --use_extracted_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-patch level & ROI-based models\n",
    "\n",
    "The 3D patch-level models compensate the absence of 3D information in the 2D slice-level approach and keep some of its advantages (low memory usage and larger sample size). This paradigm can be divided in two different frameworks:\n",
    "- `single-CNN`: one CNN is trained on all patch locations.\n",
    "- `multi-CNN`: one CNN is trained per patch location.\n",
    "For `multi-CNN` the sample size is smaller (equivalent to image level framework), however the CNNs may be more accurate as they are specialized in one patch location.\n",
    "\n",
    "ROI-based models are similar to 3D-patch but take advantage of prior knowledge on Alzheimer's disease. Indeed most of the patches are not informative as they contain parts of the brain that are not affected by the disease. Methods based on regions of interest (ROI) overcome this issue by focusing on regions which are known to be informative: the hippocampi. In this way, the complexity of the framework can be decreased as fewer inputs are used to train the networks.\n",
    "\n",
    "The 3D patch-level and ROI-based CNNs in `clinicadl` have the same architecture, including:\n",
    "* 4 convolutional layers with kernel 3x3x3,\n",
    "* 4 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "For 3D-patch level networks, it is possible to train one CNN per patch location. In this case use the subparser `multicnn` and specify the number of patches in `--num_cnn`.\n",
    "\n",
    "<img src=\"./images/ROICNN.png\">\n",
    "\n",
    "As for the 2D slice-level model, the gradients update are done based on the loss computed on the slice level. Final performances are computed on the subject level by combining the outputs of the patches or the two hippocampi of the same subject.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture. The encoder will share the same architecture than the CNN until the fully-connected layers.</p>\n",
    "    <img src=\"./images/autoencoder.png\">\n",
    "    <p>Then the weights of the encoder will be transferred in the convolutions of the CNN to initialize it before its training. This procedure is called <i>autoencoder pretraining</i>.</p>\n",
    "    <p>It is also possible to transfer weights between two CNNs with the same architecture.</p>\n",
    "    <p>For 3D-patch multi-CNNs specifically, it is possible to initialize each CNN of a multi-CNN:\n",
    "        <ul>\n",
    "        <li> with the weights of a single-CNN,</li>\n",
    "        <li> with the weights of the corresponding CNN in a multi-CNN. </li>\n",
    "    </ul>\n",
    "    <p>Transferring weights between CNNs can be useful when performing two classification tasks that are similar. This is what has been done in <font color='green'>give here the reference of AD-DL</font>: the sMCI vs pMCI classification network was initialized with the weights of the AD vs CN classification network.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>Transferring weights between tasks that are not similar enough can hurt the performance !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D-patch level models\n",
    "\n",
    "See definition of patches in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch autoencoder pretraining\n",
    "!clinicadl train patch autoencoder -h\n",
    "#!clinicadl train patch autoencoder <caps_dir> data/labels_list results/patch_autoencoder Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch single-CNN training\n",
    "!clinicadl train patch cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_single-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train patch cnn <caps_dir> data/labels_list results/patch_single-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-patch multi-CNN training\n",
    "!clinicadl train patch multicnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_autoencoder --transfer_learning_path results/patch_autoencoder\n",
    "# With single-CNN pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches --transfer_learning_path results/patch_single-cnn\n",
    "# Without pretraining\n",
    "#!clinicadl train patch multicnn <caps_dir> data/labels_list results/patch_multi-cnn Conv4_FC3 --batch_size 16 --use_extracted_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROI-based models\n",
    "\n",
    "See definition of ROI in the <font color=\"green\">extract</font> section of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based autoencoder pretraining\n",
    "!clinicadl train roi autoencoder -h\n",
    "#!clinicadl train roi autoencoder <caps_dir> data/labels_list results/roi_autoencoder Conv4_FC3 --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-based CNN training\n",
    "!clinicadl train roi cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> data/labels_list results/roi_cnn Conv4_FC3 --batch_size 16 --transfer_learning_autoencoder --transfer_learning_path results/roi_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train roi cnn <caps_dir> data/labels_list results/roi_cnn Conv4_FC3 --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-image level model\n",
    "\n",
    "In this approach, the whole MRI is used at once and the classification is performed at the imageg level. The advantage is that the spatial information is fully integrated, and it may allow to discover new knowledge on the disease. However, it requires more computational resources (especially GPU with higher memory capacity).\n",
    "\n",
    "The 3D image-level CNN in `clinicadl` is designed as follows:\n",
    "* 5 convolutional layers with kernel 3x3x3,\n",
    "* 5 max pooling layers with stride and kernel of 2 and a padding value that automatically adapts to the input feature map size.\n",
    "* 3 fully-connected layers.\n",
    "\n",
    "<img src=\"./images/imageCNN.png\">\n",
    "\n",
    "Depending on the preprocessing, the size of the fully connected layers must be adapted. This is why two models exist in `clinicadl`:\n",
    "* `Conv5_FC3` adapted to `linear` preprocessing, <font color=\"green\">names might change</font>\n",
    "* `Conv5_FC3_mni` adapted to `mni` preprocessing.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Transfer learning:</b> \n",
    "    <p>It is possible for this category to train an autoencoder derived from the CNN architecture, or to transfer weights between CNNs. See the section on patches for more details on this topic !</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image autoencoder -h\n",
    "#!clinicadl train image autoencoder <caps_dir> data/labels_list results/image_autoencoder Conv5_FC3 -gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D-image level autoencoder pretraining\n",
    "!clinicadl train image cnn -h\n",
    "\n",
    "# With autoencoder pretraining\n",
    "#!clinicadl train image cnn <caps_dir> data/labels_list results/image_cnn Conv5_FC3 --transfer_learning_autoencoder --transfer_learning_path results/image_autoencoder\n",
    "# Without pretraining\n",
    "#!clinicadl train image cnn <caps_dir> data/labels_list results/image_cnn Conv5_FC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models\n",
    "\n",
    "You want to add your custom architecture and train it with clinicadl ? Please fork and clone the [github repo](https://github.com/aramis-lab/AD-DL]) and code your model in pytorch in `clinicadl/tools/deep_learning/models` and import it in `clinicadl/tools/deep_learning/models/__init__.py` !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
