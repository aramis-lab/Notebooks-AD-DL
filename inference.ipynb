{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell if running in Google Colab\n",
    "# !pip install clinicadl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfom classification using pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<SCRIPT SRC='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></SCRIPT>\n",
    "<SCRIPT>MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}})</SCRIPT> \n",
    "\n",
    "This notebook shows how to perform classification on preprocessed data using pretrained models of [Wen et al, 2020](https://www.sciencedirect.com/science/article/abs/pii/S1361841520300591).\n",
    "\n",
    "## Structure of the pretrained models\n",
    "\n",
    "All the pretrained model folders are organized as follows:\n",
    "<pre>\n",
    "<b>results</b>\n",
    "├── commandline.json\n",
    "├── <b>fold-0</b>\n",
    "├── ...\n",
    "└── <b>fold-4</b>\n",
    "    ├── <b>models</b>\n",
    "    │      └── <b>best_balanced_accuracy</b>\n",
    "    │          └── model_best.pth.tar\n",
    "    └── <b>cnn_classification</b>\n",
    "           └── <b>best_balanced_accuracy</b>\n",
    "               └── validation_{patch|roi|slice}_level_prediction.tsv\n",
    "</pre>\n",
    "This file system is a part of the output of `clinicadl train` and `clinicadl classify` relies on three files:\n",
    "<ul>\n",
    "    <li> <code>commandline.json</code> contains all the options that were entered for training (type of input, architecture, preprocessing...)</li>\n",
    "    <li> <code>model_best.pth.tar</code> corresponds to the model selected when the best validation balanced accuracy was obtained.</li>\n",
    "    <li> <code>validation_{patch|roi|slice}_level_prediction.tsv</code> is specific to patch, roi and slice frameworks and is necessary to perform <b>soft-voting</b> thus finding the label on the image level in unbiased way. Indeed weighting the patches based on their performance of input data would bias the result as the classification framework would exploit knowledge on test data.</li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Soft voting:</b><p>\n",
    "    For classification tasks which take as input a part of the MRI volume (<i>patch, roi or slice</i>), an ensemble operation is needed to obtain the label at the image level.</p>\n",
    "    <p>For example, size and stride of 50 voxels on linear preprocessing leads to the classification of 36 patches, but they are not all equally meaningful. Patches that are in the corners of the image are mainly composed of background and skull and may be misleading, whereas patches in the brain may be more useful.</p>\n",
    "    <img src=\"./images/patches.png\">\n",
    "    <p>Then the image-level probability of AD <i>p<sup>AD</sup></i> will be:</p>\n",
    "    $$ p^{AD} = {\\sum_{i=0}^{35} bacc_i * p_i^{AD}}.$$\n",
    "    where:<ul>\n",
    "    <li> <i>p<sub>i</sub><sup>AD</sup></i> is the probability of AD for patch <i>i</i></li>\n",
    "    <li> <i>bacc<sub>i</sub></i> is the validation balanced accuracy for patch <i>i</i></li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the pretrained models\n",
    "\n",
    "\n",
    "```{warning} \n",
    "**Warning:** For the sake of the demostration, this tutoriel uses truncated versions of the models, containing only the first fold.\n",
    "```\n",
    "\n",
    "In this notebook, we propose to use 4 specific models , all of them where trained to predict a classification task AD vs CN. (The experiment corresponding to the pretrained model in eTable 4 of the paper mentioned above is shown besides):\n",
    "\n",
    "1. **3D image-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 3).\n",
    "2. **3D ROI-based model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 8).\n",
    "3. **3D patch-level model**, multi-cnn, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 14).\n",
    "4. **2D slice-level model**, pretrained with the baseline data and initialized with an autoencoder (_cf._ exp. 18).\n",
    "\n",
    "Commands in the next code cell will automatically download and uncompress these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "\"hide-input\""
    ]
   },
   "outputs": [],
   "source": [
    "# Download here the pretrained models stored online\n",
    "# Model 1\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/models/dl/models_v002/model_exp3_splits_1.tar.gz  -o model_exp3_splits_1.tar.gz\n",
    "!tar xf model_exp3_splits_1.tar.gz\n",
    "\n",
    "# Model 2\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/models/dl/models_v002/model_exp8_splits_1.tar.gz  -o model_exp8_splits_1.tar.gz\n",
    "!tar xf model_exp8_splits_1.tar.gz\n",
    "\n",
    "# Model 3\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/models/dl/models_v002/model_exp14_splits_1.tar.gz  -o model_exp14_splits_1.tar.gz\n",
    "!tar xf model_exp14_splits_1.tar.gz\n",
    "\n",
    "# Model 4\n",
    "!curl -k https://aramislab.paris.inria.fr/files/data/models/dl/models_v002/model_exp18_splits_1.tar.gz  -o model_exp18_splits_1.tar.gz\n",
    "!tar xf model_exp18_splits_1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `clinicadl classify`\n",
    "\n",
    "Running classification on a dataset is extremly simple using `clinicadl`. In this case, we will continue using the data preprocessed in the [previous notebook](./preprocessing). The models have been trained exclusively on ADNI dataset, all the subjects of OASIS-1 can be used to evaluate the model (without risking data leackage).\n",
    "\n",
    "If you ran the previous notebook, you must have a folder called `OasisCaps_example` in the current directory (Otherwise uncomment the next cell to download a local version of the necessary folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisCaps2.tar.gz -o OasisCaps2.tar.gz\n",
    "# !tar xf OasisCaps2.tar.gz\n",
    "# !curl -k https://aramislab.paris.inria.fr/files/data/databases/tuto/OasisBids.tar.gz -o OasisBids.tar.gz\n",
    "# !tar xf OasisBids.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we will classify the images using the pretrained models. The input necessary for `clinica classify` are:\n",
    "* A Caps directory (`OasisCaps_example`)\n",
    "* A tsv file with subjects/sessions to process, containing the diagnosis (`participants.tsv`).\n",
    "* The path to the pretrained model.\n",
    "\n",
    "Some optional parameters includes:\n",
    "* An output prefix for the output file, defined by the user (`-pre`)\n",
    "* The posibility of process non labeled data (without diagnosis).\n",
    "* The option to use previously extracted patches/slices.\n",
    "\n",
    "For these examples, we will use the simplest case, by adding a prefix (optional parameter `-pre`) for the output files.\n",
    "\n",
    "```{warning}\n",
    "If your computer is not equiped with a GPU card add the option `-cpu` to the command.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to generate a valid tsv file. We use the tool `clinica iotools`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinica iotools merge-tsv OasisBids_example OasisCaps_example/participants.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can run the classificator for the **image-level** model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute classify on OASIS dataset\n",
    "# Model 1\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp3_splits_1 -pre 'test_DB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of our classifier the subjects of this dataset are shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>true_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-OASIS10016</td>\n",
       "      <td>ses-M00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-OASIS10109</td>\n",
       "      <td>ses-M00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-OASIS10304</td>\n",
       "      <td>ses-M00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-OASIS10363</td>\n",
       "      <td>ses-M00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id session_id  true_label  predicted_label\n",
       "0  sub-OASIS10016    ses-M00           1                1\n",
       "1  sub-OASIS10109    ses-M00           0                0\n",
       "2  sub-OASIS10304    ses-M00           1                1\n",
       "3  sub-OASIS10363    ses-M00           0                0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp3_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test_DB_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remind that 0 corresponds to the **CN** class and 1 to the **AD**. It's also important to remember that the two last images/subjects performed badly when running the quality check step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clinica classify` produces also a file containing different metrics (accuracy, balanced accuracy, etc) for the current dataset. It can be displayed by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>ppv</th>\n",
       "      <th>npv</th>\n",
       "      <th>total_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.267146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  balanced_accuracy  sensitivity  specificity  ppv  npv  total_loss\n",
       "0       1.0                1.0          1.0          1.0  1.0  1.0    0.267146"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv(\"./model_exp3_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test_DB_image_level_metrics.tsv\", sep=\"\\t\")\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can process the dataset with all the other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 3D ROI-based model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp8_splits_1 -pre 'test_DB'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp8_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test_DB_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 3D patch-level model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp14_splits_1 -pre 'test_DB'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp14_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test_DB_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 2D slice-level model\n",
    "!clinicadl classify ./OasisCaps_example ./OasisCaps_example/participants.tsv ./model_exp18_splits_1 -pre 'test_DB'\n",
    "\n",
    "predictions = pd.read_csv(\"./model_exp18_splits_1/fold-0/cnn_classification/best_balanced_accuracy/test_DB_image_level_prediction.tsv\", sep=\"\\t\")\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
