{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format & preprocess Neuroimaging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform your dataset into BIDS using Clinica\n",
    "\n",
    "\n",
    "Before any processing, the OASIS dataset must be converted into a format friendly to neuroimaging tool. This format is called [BIDS](https://bids.neuroimaging.io/). A BIDS complient dataset consists in a specific organisation of the results obtained in neuroimaging experiment.\n",
    "\n",
    "[Clinica provides tools](http://www.clinica.run/doc/Converters/OASIS2BIDS/) to generate automatically this organisation.\n",
    "\n",
    "A command line instruction is enough to get the data in BIDS format:\n",
    "\n",
    "```bash\n",
    "\n",
    "clinica convert oasis-to-bids <dataset_directory> <clinical_data_directory> <bids_directory>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess raw images with `t1-linear` pipeline\n",
    "\n",
    "Original images are preprocessed following the pipeline described in the original [paper](https://www.sciencedirect.com/science/article/abs/pii/S1361841520300591?via%3Dihub). \n",
    "In this document, a \"mininal\" preprocessing procedure is described. This procedure performs the following operations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bias field correction** using the N4ITK method [Tustison et al., 2010].\n",
    "\n",
    "* **Affine registration** to the MNI152NLin2009cSym template [Fonov et al., 2011, 2009] in MNI space with the SyN algorithm [Avants et al., 2008].\n",
    "\n",
    "* **Cropping** the registered images in order to remove the background. The final image size is 169×208×179 with 1 mm3 isotropic voxels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Field Correction.\n",
    "MR images can be corrupted by a low frequency and smooth signal caused by magnetic field inhomogeneities. This bias field induces variations in the intensity of the same tissue in different locations of the image, which deteriorates the performance of image analysis algorithms such as registration (Vovk et al., 2007). Several methods exist to correct these intensity inhomogeneities, two popular ones being the nonparametric nonuniformity intensity normalization (N3) algorithm (Sled et al., 1998), available for example in the Freesurfer software package, and the N4 algorithm (Tustison et al., 2010) implemented in ITK.\n",
    "\n",
    "### Affine registration\n",
    "Medical image registration consists of spatially aligning two or more images, either globally (rigid and affine registration) or locally (non-rigid registration), so that voxels in corresponding positions contain comparable information. A large number of software tools have been developed for MRI-based registration, among them the SyN algorithm implemented in [ANTs](http://stnava.github.io/ANTs/). In this preprocessing, all images are aligned to a reference template (MNI152NLin2009cSym). \n",
    "\n",
    "### Cropping \n",
    "One images are aligned some unuseful voxels can be removed. For example, voxels representing the background do not contain preponderant information for the training process. On the contraty, removing the can improve computing efficiency. At the end these images have a size of 169×208×179 with 1 mm3 isotropic voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three steps are emmbeded in single pipeline provieded by Clinica. It is called [**T1_linear**](http://www.clinica.run/doc/Pipelines/T1_Linear/), and it is very simple to use:\n",
    "\n",
    "```bash\n",
    "clinica run t1-linear bids_directory caps_directory\n",
    "```\n",
    "\n",
    "The option `--crop_image` assures to perform the \"cropping\" stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!clinica run t1-linear bids_directory caps_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pipeline has been run, the necessary outputs for the next steps are saved using a specific suffix: \n",
    "    `_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.nii.gz`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, processed images from our dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert nifti files to tensors\n",
    "\n",
    "Once the dataset had been preprocessed we need to obtain files suited for the training phase.\n",
    "This task can be performed using the [Clinica pipeline `deeplearning-prepare-data`](http://www.clinica.run/doc/Pipelines/MachineLearning_Classification/)\n",
    "\n",
    "This pipeline select the preprocessed images, extract \"tensors\", and write them as output files for the entire image, each slice ot each patch.\n",
    "\n",
    "```\n",
    "clinica run deeplearning-prepare-data <caps_directory> {image,slice,patch}\n",
    "```\n",
    "\n",
    "Output files are stored into a new folder (inside the CAPS) and follows a struture like this:\n",
    "\n",
    "```\n",
    "deeplearning_prepare_data\n",
    "├── image_based\n",
    "│   └── t1_linear\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_T1w.pt\n",
    "├── patch_basedd\n",
    "│   └── t1_linear\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-0_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-1_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-2_T1w.pt\n",
    "│       ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-3_T1w.pt\n",
    "│       ├── ...\n",
    "│       ├── ...\n",
    "│       ├── ...\n",
    "│       └── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_patchsize-50_stride-50_patch-N_T1w.pt\n",
    "└── slice_based\n",
    "    └── t1_linear\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-0_T1w.pt\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-1_T1w.pt\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-2_T1w.pt\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-3_T1w.pt\n",
    "        ├── ... \n",
    "        ├── ...\n",
    "        ├── ...\n",
    "        ├── sub-<participant_label>_ses-<session_label>_T1w_space-MNI152NLin2009cSym_desc-Crop_res-1x1x1_axis-axi_channel-rgb_slice-N_T1w.pt\n",
    "```\n",
    "\n",
    "In a short, there is a folder for each feature (image, slice or patch) and inside the numerated tensor files with the corresponding feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
